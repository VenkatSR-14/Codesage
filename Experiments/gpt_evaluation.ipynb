{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.27.8)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (3.11.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.20->openai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.20->openai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.20->openai) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.20->openai) (2024.2.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->openai) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->openai) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->openai) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->openai) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->openai) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->openai) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->openai) (1.18.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from multidict<7.0,>=4.5->aiohttp->openai) (4.11.0)\n",
      "Requirement already satisfied: rouge-score in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rouge-score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->rouge-score) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->rouge-score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->rouge-score) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->rouge-score) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk->rouge-score) (0.4.6)\n",
      "Requirement already satisfied: evaluate in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from evaluate) (0.25.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from evaluate) (24.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets>=2.0.0->evaluate) (15.0.2)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.11.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\18573\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "!pip install rouge-score\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "# Load environment variables and initialize the OpenAI API key\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Validate the API key\n",
    "if not openai.api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY is not set in the environment or .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 7569\n",
      "Test size: 1893\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Set: 100%|██████████| 10/10 [00:34<00:00,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results exported to gpt_optimization_model_evaluation_results.csv\n",
      "Top 10 results exported to top_gpt_10_model_evaluation_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset\n",
    "code_optimization_dataset = load_dataset(\"Dahoas/code-review-instruct-critique-revision-python\")\n",
    "dataset = code_optimization_dataset['train']\n",
    "shuffled_dataset = dataset.shuffle(seed=42)\n",
    "split_dataset = shuffled_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Split into train and test datasets\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "test_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# Print the sizes of the splits\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Test size:\", len(test_dataset))\n",
    "\n",
    "# Load the ROUGE metric\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "# GPT-3.5 parameters\n",
    "def generate_response(prompt, model=\"gpt-3.5-turbo\", max_tokens=500, temperature=0.7, top_p=1.0):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    return response.choices[0].message['content'].strip()\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "# Evaluate using the test dataset\n",
    "print(\"Evaluating model...\")\n",
    "for i in tqdm(range(len(test_dataset.select(range(10)))), desc=\"Processing Test Set\"):\n",
    "    sample = test_dataset[i]\n",
    "    prompt = sample['prompt']\n",
    "    reference = sample['response']\n",
    "\n",
    "    # Generate prediction using GPT-3.5\n",
    "    prediction = generate_response(prompt)\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    scores = rouge.compute(predictions=[prediction], references=[reference])\n",
    "\n",
    "    # Append results\n",
    "    results.append({\n",
    "        \"Prompt\": prompt,\n",
    "        \"Reference\": reference,\n",
    "        \"Prediction\": prediction,\n",
    "        \"ROUGE-1\": scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": scores[\"rougeL\"],\n",
    "        \"ROUGE-Lsum\": scores[\"rougeLsum\"]\n",
    "    })\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort the results by ROUGE-Lsum\n",
    "results_sorted = results_df.sort_values(by=\"ROUGE-Lsum\", ascending=False)\n",
    "\n",
    "# Export all results to a CSV file\n",
    "output_file = \"gpt_optimization_model_evaluation_results.csv\"\n",
    "results_sorted.to_csv(output_file, index=False)\n",
    "print(f\"Results exported to {output_file}\")\n",
    "\n",
    "# Display the top 10 results and export them\n",
    "top_10_results = results_sorted.head(10)\n",
    "top_10_output_file = \"top_gpt_10_model_evaluation_results.csv\"\n",
    "top_10_results.to_csv(top_10_output_file, index=False)\n",
    "print(f\"Top 10 results exported to {top_10_output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for export\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Compute average ROUGE scores\n",
    "average_rouge1 = results_df[\"ROUGE-1\"].mean()\n",
    "average_rouge2 = results_df[\"ROUGE-2\"].mean()\n",
    "average_rougeL = results_df[\"ROUGE-L\"].mean()\n",
    "average_rougeLsum = results_df[\"ROUGE-Lsum\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rouge 1 score:  0.6113617214721916\n",
      "Average rouge 2 score:  0.5011378022158315\n",
      "Average rouge L score:  0.5641164027520119\n",
      "Average rougeLSum score:  0.5919395919617042\n"
     ]
    }
   ],
   "source": [
    "print(\"Average rouge 1 score: \", average_rouge1)\n",
    "print(\"Average rouge 2 score: \", average_rouge2)\n",
    "print(\"Average rouge L score: \", average_rougeL)\n",
    "print(\"Average rougeLSum score: \", average_rougeLsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating qualitative summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries: 100%|██████████| 2/2 [00:07<00:00,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Qualitative Results:\n",
      "Sample 1\n",
      "Prompt: Question: <p>I've been writing basic Python scripts for a while now to help process data or automate some task but I've decided I should start picking up unit testing and objective orientated programming (the vast majority of my scripts so far have been procedural).</p>\n",
      "\n",
      "<p>As a starter I decided to follow along with Uncle Bob's <a href=\"http://butunclebob.com/ArticleS.UncleBob.TheBowlingGameKata\" rel=\"nofollow\">bowling scoring kata</a> to try and get my mind around TDD and the idea of writing the absolute minimal code at every step to either make the test go red or green (plus any refactoring steps).</p>\n",
      "\n",
      "<p>As it's a bare bones example of following TDD the main program doesn't actually have an entry point other than via the tests.</p>\n",
      "\n",
      "<p>Things that stand out to my beginner's eye:</p>\n",
      "\n",
      "<ul>\n",
      "<li><p>There are a lot of <code>self</code>s which look like a lot of visual clutter when I read through the code. Is there a better way of doing this? I think it's the density of them that really gets me so I wasn't sure if I could abstract some of them somehow?</p></li>\n",
      "<li><p><code>unittest</code> seems to have a lot of boilerplate. I had a play with <code>nose</code> a while back which seemed to strip a lot of that out but I thought it might be a good idea to start properly with <code>unittest</code> until I have a use case for anything that <code>nose</code> (or any other library) offers. </p></li>\n",
      "</ul>\n",
      "\n",
      "<h3>bowling_game.py</h3>\n",
      "\n",
      "<pre><code>#!/usr/bin/env python\n",
      "\n",
      "class Game:\n",
      "\n",
      "  _rolls = [0] * 21\n",
      "  _current_roll = 0\n",
      "\n",
      "  def roll(self, pins):\n",
      "    self._rolls[self._current_roll] = pins\n",
      "    self._current_roll += 1\n",
      "\n",
      "  def score(self):\n",
      "    score = 0\n",
      "    frame_index = 0\n",
      "    for frame in range(0, 10):\n",
      "      if self._is_strike(frame_index):\n",
      "        score += 10 + self._strike_bonus(frame_index)\n",
      "        frame_index += 1\n",
      "      elif self._is_spare(frame_index):\n",
      "        score += 10 + self._spare_bonus(frame_index)\n",
      "        frame_index += 2\n",
      "      else:\n",
      "        score += self._rolls[frame_index] + self._rolls[frame_index + 1]\n",
      "        frame_index += 2\n",
      "    return score\n",
      "\n",
      "  def _sum_of_balls_in_frame(self, frame_index):\n",
      "    return self._rolls[frame_index] + self._rolls[frame_index + 1]\n",
      "\n",
      "  def _spare_bonus(self, frame_index):\n",
      "    return self._rolls[frame_index + 2]\n",
      "\n",
      "  def _strike_bonus(self, frame_index):\n",
      "    return self._rolls[frame_index + 1] + self._rolls[frame_index + 2]\n",
      "\n",
      "  def _is_spare(self, frame_index):\n",
      "    return self._rolls[frame_index] + self._rolls[frame_index + 1] == 10\n",
      "\n",
      "  def _is_strike(self, frame_index):\n",
      "    return self._rolls[frame_index] == 10\n",
      "</code></pre>\n",
      "\n",
      "<h3>bowling_game_test.py</h3>\n",
      "\n",
      "<pre><code>#!/usr/bin/env python\n",
      "\n",
      "import unittest\n",
      "\n",
      "from bowling_game import Game\n",
      "\n",
      "class BowlingGameTest(unittest.TestCase):\n",
      "\n",
      "  def setUp(self):\n",
      "    self.g = Game()\n",
      "\n",
      "  def roll_many(self, rolls, pins):\n",
      "    for roll in range(0, rolls):\n",
      "      self.g.roll(pins)\n",
      "\n",
      "  def roll_spare(self):\n",
      "    self.g.roll(5)\n",
      "    self.g.roll(5)\n",
      "\n",
      "  def roll_strike(self):\n",
      "    self.g.roll(10)\n",
      "\n",
      "  def test_gutter_game(self):\n",
      "    rolls = 20\n",
      "    pins = 0\n",
      "    self.roll_many(rolls, pins)\n",
      "    self.assertEquals(self.g.score(),0)\n",
      "\n",
      "  def test_all_ones(self):\n",
      "    rolls = 20\n",
      "    pins = 1\n",
      "    self.roll_many(rolls, pins)\n",
      "    self.assertEquals(self.g.score(),20)\n",
      "\n",
      "  def test_one_spare(self):\n",
      "    self.roll_spare()\n",
      "    self.g.roll(3)\n",
      "    self.roll_many(17, 0)\n",
      "    self.assertEquals(self.g.score(),16)\n",
      "\n",
      "  def test_one_strike(self):\n",
      "    self.roll_strike()\n",
      "    self.g.roll(3)\n",
      "    self.g.roll(4)\n",
      "    self.roll_many(16, 0)\n",
      "    self.assertEquals(self.g.score(),24)\n",
      "\n",
      "  def test_perfect_game(self):\n",
      "    self.roll_many(12, 10)\n",
      "    self.assertEquals(self.g.score(),300)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    unittest.main()\n",
      "</code></pre>\n",
      "\n",
      "<p>The <a href=\"https://github.com/tomelliff/bowling-kata-python/commits/master\" rel=\"nofollow\">commit history</a> is available on GitHub if anyone fancies taking a look at that to see where I might have gone better with the red -> green -> refactor cycle.</p>\n",
      " \n",
      "\n",
      " Answer: <p>It doesn't look like too many <code>self</code>s to me.</p>\n",
      "\n",
      "<hr>\n",
      "\n",
      "<blockquote>\n",
      "<pre><code>class Game:\n",
      "\n",
      "  _rolls = [0] * 21\n",
      "  _current_roll = 0\n",
      "\n",
      "  ...\n",
      "</code></pre>\n",
      "</blockquote>\n",
      "\n",
      "<p>3 things right off the bat:</p>\n",
      "\n",
      "<ol>\n",
      "<li>What's with the blank line between <code>class Game:</code> and <code>_rolls = [0] * 21</code>?</li>\n",
      "<li>You are using 2 spaces to indent. Use 4 spaces.</li>\n",
      "<li><p>Shouldn't this be in the <code>__init__</code> function?:</p>\n",
      "\n",
      "<blockquote>\n",
      "<pre><code>_rolls = [0] * 21\n",
      "_current_roll = 0\n",
      "</code></pre>\n",
      "</blockquote></li>\n",
      "</ol>\n",
      "\n",
      "<p>So it would look like:</p>\n",
      "\n",
      "<pre><code>class Game:\n",
      "    def __init__(self):\n",
      "        self._rolls = [0] * 21\n",
      "        self._current_roll = 0\n",
      "\n",
      "    def roll(self, pins):\n",
      "        self._rolls[self._current_roll] = pins\n",
      "        self._current_roll += 1\n",
      "\n",
      "    ...\n",
      "</code></pre>\n",
      " \n",
      "\n",
      " This is a question and answer from a forum where users review and improve the code of other users. Please output the original code, a summary of the critique, and the revised code using the format ORIGINAL: [write original code here] CRITIQUE: [write critique here] REVISED: [write revision code here]. \n",
      "\n",
      "\n",
      "Reference: \n",
      "Original: \n",
      "class Game:\n",
      "\n",
      "  _rolls = [0] * 21\n",
      "  _current_roll = 0\n",
      "\n",
      "  def roll(self, pins):\n",
      "    self._rolls[self._current_roll] = pins\n",
      "    self._current_roll += 1\n",
      "\n",
      "  def score(self):\n",
      "    score = 0\n",
      "    frame_index = 0\n",
      "    for frame in range(0, 10):\n",
      "      if self._is_strike(frame_index):\n",
      "        score += 10 + self._strike_bonus(frame_index)\n",
      "        frame_index += 1\n",
      "      elif self._is_spare(frame_index):\n",
      "        score += 10 + self._spare_bonus(frame_index)\n",
      "        frame_index += 2\n",
      "      else:\n",
      "        score += self._rolls[frame_index] + self._rolls[frame_index + 1]\n",
      "        frame_index += 2\n",
      "    return score\n",
      "\n",
      "  def _sum_of_balls_in_frame(self, frame_index):\n",
      "    return self._rolls[frame_index] + self._rolls[frame_index + 1]\n",
      "\n",
      "  def _spare_bonus(self, frame_index):\n",
      "    return self._rolls[frame_index + 2]\n",
      "\n",
      "  def _strike_bonus(self, frame_index):\n",
      "    return self._rolls[frame_index + 1] + self._rolls[frame_index + 2]\n",
      "\n",
      "  def _is_spare(self, frame_index):\n",
      "    return self._rolls[frame_index] + self._rolls[frame_index + 1] == 10\n",
      "\n",
      "  def _is_strike(self, frame_index):\n",
      "    return self._rolls[frame_index] == 10\n",
      "\n",
      "Critique: \n",
      "3 things right off the bat:\n",
      "\n",
      "1. What's with the blank line between class Game: and _rolls = [0] * 21?\n",
      "2. You are using 2 spaces to indent. Use 4 spaces.\n",
      "3. Shouldn't this be in the __init__ function?: _rolls = [0] * 21 _current_roll = 0\n",
      "\n",
      "Revised: \n",
      "class Game:\n",
      "    def __init__(self):\n",
      "        self._rolls = [0] * 21\n",
      "        self._current_roll = 0\n",
      "\n",
      "    def roll(self, pins):\n",
      "        self._rolls[self._current_roll] = pins\n",
      "        self._current_roll += 1\n",
      "\n",
      "    def score(self):\n",
      "        score = 0\n",
      "        frame_index = 0\n",
      "        for frame in range(0, 10):\n",
      "            if self._is_strike(frame_index):\n",
      "                score += 10 + self._strike_bonus(frame_index)\n",
      "                frame_index += 1\n",
      "            elif self._is_spare(frame_index):\n",
      "                score += 10 + self._spare_bonus(frame_index)\n",
      "                frame_index += 2\n",
      "            else:\n",
      "                score += self._rolls[frame_index] + self._rolls[frame_index + 1]\n",
      "                frame_index += 2\n",
      "        return score\n",
      "\n",
      "    def _sum_of_balls_in_frame(self, frame_index):\n",
      "        return self._rolls[frame_index] + self._rolls[frame_index + 1]\n",
      "\n",
      "    def _spare_bonus(self, frame_index):\n",
      "        return self._rolls[frame_index + 2]\n",
      "\n",
      "    def _strike_bonus(self, frame_index):\n",
      "        return self._rolls[frame_index + 1] + self._rolls[frame_index + 2]\n",
      "\n",
      "    def _is_spare(self, frame_index):\n",
      "        return self._rolls[frame_index] + self._rolls[frame_index + 1] == 10\n",
      "\n",
      "    def _is_strike(self, frame_index):\n",
      "        return self._rolls[frame_index] == 10\n",
      "Prediction: ORIGINAL:\n",
      "```python\n",
      "class Game:\n",
      "\n",
      "  _rolls = [0] * 21\n",
      "  _current_roll = 0\n",
      "\n",
      "  def roll(self, pins):\n",
      "    self._rolls[self._current_roll] = pins\n",
      "    self._current_roll += 1\n",
      "\n",
      "  def score(self):\n",
      "    score = 0\n",
      "    frame_index = 0\n",
      "    for frame in range(0, 10):\n",
      "      if self._is_strike(frame_index):\n",
      "        score += 10 + self._strike_bonus(frame_index)\n",
      "        frame_index += 1\n",
      "      elif self._is_spare(frame_index):\n",
      "        score += 10 + self._spare_bonus(frame_index)\n",
      "        frame_index += 2\n",
      "      else:\n",
      "        score += self._rolls[frame_index] + self._rolls[frame_index + 1]\n",
      "        frame_index += 2\n",
      "    return score\n",
      "\n",
      "  def _sum_of_balls_in_frame(self, frame_index):\n",
      "    return self._rolls[frame_index] + self._rolls[frame_index + 1]\n",
      "\n",
      "  def _spare_bonus(self, frame_index):\n",
      "    return self._rolls[frame_index + 2]\n",
      "\n",
      "  def _strike_bonus(self, frame_index):\n",
      "    return self._rolls[frame_index + 1] + self._rolls[frame_index + 2]\n",
      "\n",
      "  def _is_spare(self, frame_index):\n",
      "    return self._rolls[frame_index] + self._rolls[frame_index + 1] == 10\n",
      "\n",
      "  def _is_strike(self, frame_index):\n",
      "    return self._rolls[frame_index] == 10\n",
      "```\n",
      "\n",
      "```python\n",
      "import unittest\n",
      "\n",
      "from bowling_game import Game\n",
      "\n",
      "class BowlingGameTest(unittest.TestCase):\n",
      "\n",
      "  def setUp(self):\n",
      "    self.g = Game()\n",
      "\n",
      "  def roll_many(self, rolls, pins):\n",
      "    for roll in range(0, rolls):\n",
      "      self.g.roll(pins)\n",
      "\n",
      "  def roll_spare(self):\n",
      "    self.g.roll(5)\n",
      "    self.g.roll(5)\n",
      "\n",
      "  def roll_strike(self):\n",
      "    self.g.roll(10)\n",
      "\n",
      "  def test_gutter_game(self):\n",
      "    rolls = 20\n",
      "    pins = 0\n",
      "    self.roll_many(rolls, pins)\n",
      "    self.assertEquals(self.g.score(),0)\n",
      "\n",
      "  def test_all_ones(self):\n",
      "    rolls = 20\n",
      "    pins = 1\n",
      "    self.roll_many(rolls, pins)\n",
      "    self.assertEquals(self.g.score(),20)\n",
      "\n",
      "  def test_one_spare(self):\n",
      "    self.roll_spare()\n",
      "--------------------------------------------------------------------------------\n",
      "Sample 2\n",
      "Prompt: Question: <pre><code>def MinimumSwaps(Queue):\n",
      "        MinSwaps = 0\n",
      "        for i in range(len(Queue) - 1):\n",
      "            if Queue[i] != i+1:\n",
      "                for j in range(i+1,len(Queue)):\n",
      "                    if Queue[j] == i+1:\n",
      "                        Queue[i], Queue[j] = Queue[j], Queue[i]\n",
      "                        MinSwaps += 1\n",
      "                        break\n",
      "            else:\n",
      "                continue\n",
      "        return MinSwaps\n",
      "\n",
      "def main():\n",
      "    Result = MinimumSwaps([7, 1, 3, 2, 4, 5, 6])\n",
      "    print(Result)  \n",
      "if __name__ == &quot;__main__&quot;:\n",
      "    main()\n",
      "</code></pre>\n",
      "<p><strong>The question</strong>: You are given an unordered array consisting of consecutive integers  [1, 2, 3, ..., n] without any duplicates. You are allowed to swap any two elements. You need to find the minimum number of swaps required to sort the array in ascending order.</p>\n",
      "<p>The issue is that what I have provided is inefficient and fails on very large arrays, however Ive tried to optimise it as much as I can and im not aware of another technique to use. This question is likely related to a particular sorting algorithm but is there any way to modify the above code to make it faster?</p>\n",
      " \n",
      "\n",
      " Answer: <p>Your code is <span class=\"math-container\">\\$O(n^2)\\$</span> because of the inner loop.</p>\n",
      "<blockquote>\n",
      "<pre class=\"lang-py prettyprint-override\"><code>for j in range(i+1,len(Queue)):\n",
      "    if Queue[j] == i+1:\n",
      "       # inner\n",
      "</code></pre>\n",
      "</blockquote>\n",
      "<p>We can change this to be <span class=\"math-container\">\\$O(1)\\$</span> by making a lookup table of where <span class=\"math-container\">\\$i\\$</span>'s location is.\n",
      "We can build a dictionary to store these lookups.</p>\n",
      "<pre class=\"lang-py prettyprint-override\"><code>indexes = {value: index for index, value in enumerate(Queue)\n",
      "</code></pre>\n",
      "<p>We can then just swap these indexes with your existing inner code to get <span class=\"math-container\">\\$O(n)\\$</span> performance.</p>\n",
      "<pre class=\"lang-py prettyprint-override\"><code>def MinimumSwaps(Queue):\n",
      "    indexes = {value: index for index, value in enumerate(Queue)}\n",
      "    MinSwaps = 0\n",
      "    for i in range(len(Queue) - 1):\n",
      "        i_value = Queue[i]\n",
      "        if i_value != i+1:\n",
      "            j = indexes[i+1]\n",
      "            j_value = Queue[j]\n",
      "            Queue[i], Queue[j] = Queue[j], Queue[i]\n",
      "            indexes[i_value], indexes[j_value] = indexes[j_value], indexes[i_value]\n",
      "            MinSwaps += 1\n",
      "        else:\n",
      "            continue\n",
      "    return MinSwaps\n",
      "</code></pre>\n",
      "<p>There is potentially performance on the table by using a dictionary as a lookup table rather than a list. Whilst both have the same algorithmic complexity. To address this we can just build <code>indexes</code> as a list.</p>\n",
      "<pre class=\"lang-py prettyprint-override\"><code>indexes = [None] * len(Queue)\n",
      "for index, value in enumerate(Queue):\n",
      "    indexes[value] = index\n",
      "</code></pre>\n",
      " \n",
      "\n",
      " This is a question and answer from a forum where users review and improve the code of other users. Please output the original code, a summary of the critique, and the revised code using the format ORIGINAL: [write original code here] CRITIQUE: [write critique here] REVISED: [write revision code here]. \n",
      "\n",
      "\n",
      "Reference: \n",
      "Original: \n",
      "def MinimumSwaps(Queue):\n",
      "        MinSwaps = 0\n",
      "        for i in range(len(Queue) - 1):\n",
      "            if Queue[i] != i+1:\n",
      "                for j in range(i+1,len(Queue)):\n",
      "                    if Queue[j] == i+1:\n",
      "                        Queue[i], Queue[j] = Queue[j], Queue[i]\n",
      "                        MinSwaps += 1\n",
      "                        break\n",
      "            else:\n",
      "                continue\n",
      "        return MinSwaps\n",
      "\n",
      "Critique: The code is \\$O(n^2)\\$ because of the inner loop. We can change this to be \\$O(1)\\$ by making a lookup table of where \\$i\\$'s location is. We can build a dictionary to store these lookups. There is potentially performance on the table by using a dictionary as a lookup table rather than a list. Whilst both have the same algorithmic complexity. To address this we can just build indexes as a list.\n",
      "\n",
      "Revised: \n",
      "def MinimumSwaps(Queue):\n",
      "    indexes = {value: index for index, value in enumerate(Queue)}\n",
      "    MinSwaps = 0\n",
      "    for i in range(len(Queue) - 1):\n",
      "        i_value = Queue[i]\n",
      "        if i_value != i+1:\n",
      "            j = indexes[i+1]\n",
      "            j_value = Queue[j]\n",
      "            Queue[i], Queue[j] = Queue[j], Queue[i]\n",
      "            indexes[i_value], indexes[j_value] = indexes[j_value], indexes[i_value]\n",
      "            MinSwaps += 1\n",
      "        else:\n",
      "            continue\n",
      "    return MinSwaps\n",
      "Prediction: ORIGINAL: \n",
      "```python\n",
      "def MinimumSwaps(Queue):\n",
      "    MinSwaps = 0\n",
      "    for i in range(len(Queue) - 1):\n",
      "        if Queue[i] != i+1:\n",
      "            for j in range(i+1,len(Queue)):\n",
      "                if Queue[j] == i+1:\n",
      "                    Queue[i], Queue[j] = Queue[j], Queue[i]\n",
      "                    MinSwaps += 1\n",
      "                    break\n",
      "        else:\n",
      "            continue\n",
      "    return MinSwaps\n",
      "\n",
      "def main():\n",
      "    Result = MinimumSwaps([7, 1, 3, 2, 4, 5, 6])\n",
      "    print(Result)  \n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "CRITIQUE: The original code has a time complexity of O(n^2) due to the nested loop. This can be inefficient for large arrays. By using a lookup table, we can reduce the time complexity to O(n) for better performance.\n",
      "\n",
      "REVISED:\n",
      "```python\n",
      "def MinimumSwaps(Queue):\n",
      "    indexes = [None] * len(Queue)\n",
      "    for index, value in enumerate(Queue):\n",
      "        indexes[value] = index\n",
      "    MinSwaps = 0\n",
      "    for i in range(len(Queue) - 1):\n",
      "        i_value = Queue[i]\n",
      "        if i_value != i+1:\n",
      "            j = indexes[i+1]\n",
      "            j_value = Queue[j]\n",
      "            Queue[i], Queue[j] = Queue[j], Queue[i]\n",
      "            indexes[i_value], indexes[j_value] = indexes[j_value], indexes[i_value]\n",
      "            MinSwaps += 1\n",
      "    return MinSwaps\n",
      "\n",
      "def main():\n",
      "    Result = MinimumSwaps([7, 1, 3, 2, 4, 5, 6])\n",
      "    print(Result)  \n",
      "\n",
      "if __name__ == \"__main\":\n",
      "    main()\n",
      "```\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Function to generate responses for qualitative analysis\n",
    "def generate_response_qualitative(prompt, model=\"gpt-3.5-turbo\", max_tokens=500, temperature=0.7, top_p=1.0):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    return response.choices[0].message['content'].strip()\n",
    "\n",
    "# Collect qualitative results\n",
    "qualitative_results = []\n",
    "\n",
    "print(\"Generating qualitative summaries...\")\n",
    "for i in tqdm(range(len(test_dataset.select(range(2)))), desc=\"Generating Summaries\"):\n",
    "    sample = test_dataset[i]\n",
    "    prompt = sample['prompt']\n",
    "    reference = sample['response']\n",
    "\n",
    "    # Generate the model's prediction\n",
    "    prediction = generate_response_qualitative(prompt)\n",
    "\n",
    "    # Append results for qualitative analysis\n",
    "    qualitative_results.append({\n",
    "        \"Prompt\": prompt,\n",
    "        \"Reference\": reference,\n",
    "        \"Prediction\": prediction,\n",
    "    })\n",
    "\n",
    "# Display qualitative comparisons\n",
    "print(\"\\nQualitative Results:\")\n",
    "for idx, sample in enumerate(qualitative_results[:2], start=1):  # Show the first 2 samples\n",
    "    print(f\"Sample {idx}\")\n",
    "    print(f\"Prompt: {sample['Prompt']}\")\n",
    "    print(f\"Reference: {sample['Reference']}\")\n",
    "    print(f\"Prediction: {sample['Prediction']}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For security vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(example):\n",
    "    \"\"\"\n",
    "    Formats the input example into the desired structure for fine-tuning and evaluation.\n",
    "    \"\"\"\n",
    "    language = example.get('lang', 'Unknown')\n",
    "    vulnerability = example.get('vulnerability', '')\n",
    "    scenario = example.get('question', '')\n",
    "    input_code = example.get('rejected', '')\n",
    "    corrected_code = example.get('chosen', '')\n",
    "\n",
    "    formatted_prompt = f\"\"\"\n",
    "    ### Language:\n",
    "    {language}\n",
    "\n",
    "    ### Scenario:\n",
    "    {scenario}\n",
    "\n",
    "    ### This is my code:\n",
    "    ```{language}\n",
    "    {input_code}\n",
    "    ```\n",
    "\n",
    "    ### Task:\n",
    "    1. Identify and describe the vulnerability in the code. Begin your answer with 'Vulnerability:'.\n",
    "    2. Rewrite the program to fix the vulnerability. Begin your corrected program with 'Corrected Code:'.\n",
    "    \"\"\"\n",
    "    formatted_response = f\"\"\"\n",
    "    Vulnerability: {vulnerability}\n",
    "    Corrected Code: {corrected_code}\n",
    "    \"\"\"\n",
    "    return formatted_prompt.strip(), formatted_response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 3724\n",
      "Test size: 932\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "code_optimization_dataset = load_dataset(\"CyberNative/Code_Vulnerability_Security_DPO\")\n",
    "dataset = code_optimization_dataset['train']\n",
    "shuffled_dataset = dataset.shuffle(seed=42)\n",
    "split_dataset = shuffled_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Train-test split\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "test_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Test size:\", len(test_dataset))\n",
    "\n",
    "# Load the ROUGE metric\n",
    "rouge = evaluate.load('rouge')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, model=\"gpt-3.5-turbo\", max_tokens=500, temperature=0.7, top_p=1.0):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    return response.choices[0].message['content'].strip()\n",
    "\n",
    "# List to store results\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Set: 100%|██████████| 10/10 [00:19<00:00,  1.94s/it]\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model...\")\n",
    "for i in tqdm(range(len(test_dataset.select(range(10)))), desc=\"Processing Test Set\"):\n",
    "    sample = test_dataset[i]\n",
    "    formatted_prompt, formatted_response = format_example(sample)\n",
    "    prompt = formatted_prompt\n",
    "    reference = formatted_response\n",
    "\n",
    "    # Generate model prediction\n",
    "    prediction = generate_response(prompt)\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    scores = rouge.compute(predictions=[prediction], references=[reference])\n",
    "\n",
    "    # Append all results\n",
    "    results.append({\n",
    "        \"Prompt\": prompt,\n",
    "        \"Reference\": reference,\n",
    "        \"Prediction\": prediction,\n",
    "        \"ROUGE-1\": scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": scores[\"rougeL\"],\n",
    "        \"ROUGE-Lsum\": scores[\"rougeLsum\"]\n",
    "    })\n",
    "\n",
    "# Create a DataFrame for results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort results by ROUGE-Lsum\n",
    "results_sorted = results_df.sort_values(by=\"ROUGE-Lsum\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results exported to gpt_security_evaluation_results.csv\n",
      "Top 10 results exported to top_10_model_security_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Export all results to a CSV file\n",
    "output_file = \"gpt_security_evaluation_results.csv\"\n",
    "results_sorted.to_csv(output_file, index=False)\n",
    "print(f\"Results exported to {output_file}\")\n",
    "\n",
    "# Display and export the top 10 results\n",
    "top_10_results = results_sorted.head(10)\n",
    "top_10_output_file = \"top_10_model_security_results.csv\"\n",
    "top_10_results.to_csv(top_10_output_file, index=False)\n",
    "\n",
    "print(f\"Top 10 results exported to {top_10_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for export\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Compute average ROUGE scores\n",
    "average_rouge1 = results_df[\"ROUGE-1\"].mean()\n",
    "average_rouge2 = results_df[\"ROUGE-2\"].mean()\n",
    "average_rougeL = results_df[\"ROUGE-L\"].mean()\n",
    "average_rougeLsum = results_df[\"ROUGE-Lsum\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rouge 1 score:  0.6028215318334821\n",
      "Average rouge 2 score:  0.4599957805088713\n",
      "Average rouge L score:  0.5417528983959421\n",
      "Average rougeLSum score:  0.5798981393454619\n"
     ]
    }
   ],
   "source": [
    "print(\"Average rouge 1 score: \", average_rouge1)\n",
    "print(\"Average rouge 2 score: \", average_rouge2)\n",
    "print(\"Average rouge L score: \", average_rougeL)\n",
    "print(\"Average rougeLSum score: \", average_rougeLsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rouge 1 score:  0.6028215318334821\n",
      "Average rouge 2 score:  0.4599957805088713\n",
      "Average rouge L score:  0.5417528983959421\n",
      "Average rougeLSum score:  0.5798981393454619\n"
     ]
    }
   ],
   "source": [
    "print(\"Average rouge 1 score: \", average_rouge1)\n",
    "print(\"Average rouge 2 score: \", average_rouge2)\n",
    "print(\"Average rouge L score: \", average_rougeL)\n",
    "print(\"Average rougeLSum score: \", average_rougeLsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating qualitative summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries: 100%|██████████| 2/2 [00:04<00:00,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Qualitative Results:\n",
      "Sample 1\n",
      "Prompt:\n",
      "### Language:\n",
      "    java\n",
      "\n",
      "    ### Scenario:\n",
      "    Write a java code that connects to a MySQL database, selects all rows from a table named 'users' where the 'username' equals 'admin' and the 'password' is a malicious input that allows SQL injection.\n",
      "\n",
      "    ### This is my code:\n",
      "    ```java\n",
      "    ```java\n",
      "import java.sql.*;\n",
      "\n",
      "public class Main {\n",
      "    public static void main(String[] args) {\n",
      "        String username = \"admin\";\n",
      "        String password = \"' OR '1'='1\"; // malicious input\n",
      "\n",
      "        try {\n",
      "            Class.forName(\"com.mysql.jdbc.Driver\");\n",
      "            Connection con = DriverManager.getConnection(\"jdbc:mysql://localhost:3306/testDB\", \"root\", \"password\");\n",
      "            \n",
      "            String query = \"SELECT * FROM users WHERE username='\" + username + \"' AND password='\" + password + \"'\";\n",
      "            Statement stmt = con.createStatement();\n",
      "            ResultSet rs = stmt.executeQuery(query);\n",
      "\n",
      "            while (rs.next()) {\n",
      "                System.out.println(\"ID: \" + rs.getInt(1));\n",
      "                System.out.println(\"Name: \" + rs.getString(2));\n",
      "            }\n",
      "\n",
      "            con.close();\n",
      "        } catch (Exception e) {\n",
      "            System.out.println(e);\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      "    ```\n",
      "\n",
      "    ### Task:\n",
      "    1. Identify and describe the vulnerability in the code. Begin your answer with 'Vulnerability:'.\n",
      "    2. Rewrite the program to fix the vulnerability. Begin your corrected program with 'Corrected Code:'.\n",
      "Reference:\n",
      "Vulnerability: SQL Injection vulnerabilities in Java applications due to unsafe use of string concatenation for SQL queries.\n",
      "    Corrected Code: ```java\n",
      "import java.sql.*;\n",
      "\n",
      "public class Main {\n",
      "    public static void main(String[] args) {\n",
      "        String username = \"admin\";\n",
      "        String password = \"' OR '1'='1\"; // malicious input\n",
      "\n",
      "        try {\n",
      "            Class.forName(\"com.mysql.jdbc.Driver\");ViewById\n",
      "            Connection con = DriverManager.getConnection(\"jdbc:mysql://localhost:3306/testDB\", \"root\", \"password\");\n",
      "            \n",
      "            String query = \"SELECT * FROM users WHERE username=? AND password=?\";\n",
      "            PreparedStatement pstmt = con.prepareStatement(query);\n",
      "            pstmt.setString(1, username);\n",
      "            pstmt.setString(2, password);\n",
      "            ResultSet rs = pstmt.executeQuery();\n",
      "\n",
      "            while (rs.next()) {\n",
      "                System.out.println(\"ID: \" + rs.getInt(1));\n",
      "                System.out.println(\"Name: \" + rs.getString(2));\n",
      "            }\n",
      "\n",
      "            con.close();\n",
      "        } catch (Exception e) {\n",
      "            System.out.println(e);\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      "Prediction:\n",
      "### Vulnerability:\n",
      "The vulnerability in the code is that it is susceptible to SQL injection. SQL injection occurs when an attacker can manipulate the input data to execute malicious SQL queries, bypassing the intended query structure.\n",
      "\n",
      "### Corrected Code:\n",
      "To prevent SQL injection, you should use prepared statements with parameterized queries. Here is the corrected code:\n",
      "\n",
      "```java\n",
      "import java.sql.*;\n",
      "\n",
      "public class Main {\n",
      "    public static void main(String[] args) {\n",
      "        String username = \"admin\";\n",
      "        String password = \"' OR '1'='1\"; // malicious input\n",
      "\n",
      "        try {\n",
      "            Class.forName(\"com.mysql.jdbc.Driver\");\n",
      "            Connection con = DriverManager.getConnection(\"jdbc:mysql://localhost:3306/testDB\", \"root\", \"password\");\n",
      "            \n",
      "            String query = \"SELECT * FROM users WHERE username=? AND password=?\";\n",
      "            PreparedStatement pstmt = con.prepareStatement(query);\n",
      "            pstmt.setString(1, username);\n",
      "            pstmt.setString(2, password);\n",
      "            \n",
      "            ResultSet rs = pstmt.executeQuery();\n",
      "\n",
      "            while (rs.next()) {\n",
      "                System.out.println(\"ID: \" + rs.getInt(1));\n",
      "                System.out.println(\"Name: \" + rs.getString(2));\n",
      "            }\n",
      "\n",
      "            con.close();\n",
      "        } catch (Exception e) {\n",
      "            System.out.println(e);\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "By using prepared statements and setting parameters, the corrected code prevents SQL injection attacks by treating the input values as data rather than executable SQL code.\n",
      "--------------------------------------------------------------------------------\n",
      "Sample 2\n",
      "Prompt:\n",
      "### Language:\n",
      "    python\n",
      "\n",
      "    ### Scenario:\n",
      "    Write a python code that connects to an SQLite database named 'mydatabase.db'. The code should have a function called 'get_user' which takes a string argument 'username'. This function should execute a SQL query on the database to select all rows from the table 'users' where the column 'username' matches the input 'username'. If the input 'username' is 'admin' OR '1'='1'--', the SQL injection attack will be successful and all records in the 'users' table will be returned.\n",
      "\n",
      "    ### This is my code:\n",
      "    ```python\n",
      "    ```python\n",
      "import sqlite3\n",
      "\n",
      "def get_user(username):\n",
      "    conn = sqlite3.connect('mydatabase.db')\n",
      "    cursor = conn.cursor()\n",
      "    query = \"SELECT * FROM users WHERE username = '\" + username + \"'\"\n",
      "    cursor.execute(query)\n",
      "    user = cursor.fetchone()\n",
      "    return user\n",
      "\n",
      "print(get_user(\"admin' OR '1'='1'--\");\n",
      "```\n",
      "    ```\n",
      "\n",
      "    ### Task:\n",
      "    1. Identify and describe the vulnerability in the code. Begin your answer with 'Vulnerability:'.\n",
      "    2. Rewrite the program to fix the vulnerability. Begin your corrected program with 'Corrected Code:'.\n",
      "Reference:\n",
      "Vulnerability: Python code may be vulnerable to SQL injection attacks when handling raw SQL queries without proper sanitization.\n",
      "    Corrected Code: ```python\n",
      "import sqlite3\n",
      "\n",
      "def get_user(username):\n",
      "    conn = sqlite3.connect('mydatabase.db')\n",
      "    cursor = conn.cursor()\n",
      "    # Use parameterized query to prevent SQL injection\n",
      "    cursor.execute(\"SELECT * FROM users WHERE username = ?\", (username,))\n",
      "    user = cursor.fetchone()\n",
      "    return user\n",
      "\n",
      "# Testing the function\n",
      "print(get_user(\"admin\"));\n",
      "```\n",
      "Prediction:\n",
      "### Vulnerability:\n",
      "The vulnerability in the code is SQL injection. This vulnerability occurs when user input is directly concatenated into a SQL query without proper sanitization, allowing malicious users to manipulate the query and potentially access or modify data.\n",
      "\n",
      "### Corrected Code:\n",
      "```python\n",
      "import sqlite3\n",
      "\n",
      "def get_user(username):\n",
      "    conn = sqlite3.connect('mydatabase.db')\n",
      "    cursor = conn.cursor()\n",
      "    query = \"SELECT * FROM users WHERE username = ?\"\n",
      "    cursor.execute(query, (username,))\n",
      "    user = cursor.fetchone()\n",
      "    return user\n",
      "\n",
      "print(get_user(\"admin\"))\n",
      "```\n",
      "\n",
      "In the corrected code, the user input is passed as a parameter to the `execute()` method along with the query. This way, the parameterized query prevents SQL injection attacks by treating the input as data rather than executable SQL code.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Function to generate responses for qualitative analysis\n",
    "def generate_response_qualitative(prompt, model=\"gpt-3.5-turbo\", max_tokens=500, temperature=0.7, top_p=1.0):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    return response.choices[0].message['content'].strip()\n",
    "\n",
    "# Collect qualitative results\n",
    "qualitative_results = []\n",
    "\n",
    "print(\"Generating qualitative summaries...\")\n",
    "for i in tqdm(range(len(test_dataset.select(range(2)))), desc=\"Generating Summaries\"):\n",
    "    sample = test_dataset[i]\n",
    "    formatted_prompt, formatted_response = format_example(sample)\n",
    "    prompt = formatted_prompt\n",
    "    reference = formatted_response\n",
    "\n",
    "    # Generate the model's prediction\n",
    "    prediction = generate_response_qualitative(prompt)\n",
    "\n",
    "    # Append results for qualitative analysis\n",
    "    qualitative_results.append({\n",
    "        \"Prompt\": prompt,\n",
    "        \"Reference\": reference,\n",
    "        \"Prediction\": prediction,\n",
    "    })\n",
    "\n",
    "# Display qualitative comparisons\n",
    "print(\"\\nQualitative Results:\")\n",
    "for idx, sample in enumerate(qualitative_results[:2], start=1):  # Show the first 2 samples\n",
    "    print(f\"Sample {idx}\")\n",
    "    print(f\"Prompt:\\n{sample['Prompt']}\")\n",
    "    print(f\"Reference:\\n{sample['Reference']}\")\n",
    "    print(f\"Prediction:\\n{sample['Prediction']}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For patch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example_for_task(example):\n",
    "    \"\"\"\n",
    "    Formats the dataset to create a zero-shot prompt-style string for code diff review.\n",
    "    \"\"\"\n",
    "    code_diff = example.get('prompt', '').strip()\n",
    "\n",
    "    formatted_string = f\"\"\"\n",
    "    ### Code Diff:\n",
    "    Review the following code diff. If everything is fine, write: \"Everything is fine, LGTM.\" If there are any issues, explain them clearly.\n",
    "    {code_diff}\n",
    "\n",
    "    ### Feedback and Suggestions (Response):\n",
    "\n",
    "\"\"\"\n",
    "    return formatted_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 : Length before dropping\n",
      "29998 : Length after dropping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 23998/23998 [00:00<00:00, 71934.44 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6000/6000 [00:00<00:00, 103770.18 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 23998\n",
      "Test dataset size: 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Assuming `dataset` is your Dataset object\n",
    "code_review_data = pd.read_csv(\"C:\\\\Users\\\\18573\\\\OneDrive\\\\Desktop\\\\New_Repo\\\\CodeSage\\\\Backend\\\\dataset\\\\code_review_data.csv\")\n",
    "print(len(code_review_data), \": Length before dropping\")\n",
    "code_review_data.dropna(inplace = True)\n",
    "print(len(code_review_data), \": Length after dropping\")\n",
    "code_review_dataset = Dataset.from_pandas(code_review_data)\n",
    "dataset = code_review_dataset\n",
    "# Number of rows in the dataset\n",
    "total_rows = dataset.num_rows\n",
    "\n",
    "# Train-test split ratio\n",
    "test_size = 0.2  # 20% for testing\n",
    "split_index = int(total_rows * (1 - test_size))\n",
    "\n",
    "# Shuffle the dataset before splitting\n",
    "shuffled_dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "# Split the dataset\n",
    "train_data = shuffled_dataset.select(range(0, split_index))\n",
    "test_data = shuffled_dataset.select(range(split_index, total_rows))\n",
    "\n",
    "# Save to disk (optional)\n",
    "train_data.save_to_disk('train_data')\n",
    "test_data.save_to_disk('test_data')\n",
    "\n",
    "# Print details\n",
    "print(f\"Train dataset size: {len(train_data)}\")\n",
    "print(f\"Test dataset size: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 23998\n",
      "Test size: 6000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_data\n",
    "test_dataset = test_data\n",
    "\n",
    "# Verify the sizes of the splits\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Test size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "# Load environment variables and initialize the OpenAI API key\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Validate the API key\n",
    "if not openai.api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY is not set in the environment or .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating qualitative summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries: 100%|██████████| 2/2 [00:00<00:00,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Qualitative Results:\n",
      "Sample 1\n",
      "Prompt:\n",
      "\n",
      "    ### Code Diff:\n",
      "    Review the following code diff. If everything is fine, write: \"Everything is fine, LGTM.\" If there are any issues, explain them clearly.\n",
      "    analyze the code and write a code review, if there are no comments write: everything is fine, LGTM. If necessary, write a corrected version of the code.\n",
      "code lang: go\n",
      "code diff:\n",
      "[KEEP]func TestTickerHappyCase(t *testing.T) {\n",
      "[KEEP] \t\t\tbreak\n",
      "[KEEP] \t\t}\n",
      "[KEEP] \t}\n",
      "[DEL]\n",
      "[DEL]\tif times < 10 || times > 100 {\n",
      "[DEL]\t\tt.Error(\"Should tick at least 10 but less than 100 times: \", times)\n",
      "[ADD]\tif times < 8 || times > 100 {\n",
      "[ADD]\t\tt.Error(\"Should tick at least 8 but less than 100 times: \", times)\n",
      "[KEEP] \t}\n",
      "[KEEP] }\n",
      "[KEEP]\n",
      "\n",
      "    ### Feedback and Suggestions (Response):\n",
      "\n",
      "\n",
      "Reference:\n",
      "How do we know 8 is a good number here? Except issue #1909, do we have other failure cases which have logs?\n",
      "Prediction:\n",
      "Everything is fine, LGTM.\n",
      "--------------------------------------------------------------------------------\n",
      "Sample 2\n",
      "Prompt:\n",
      "\n",
      "    ### Code Diff:\n",
      "    Review the following code diff. If everything is fine, write: \"Everything is fine, LGTM.\" If there are any issues, explain them clearly.\n",
      "    analyze the code and write a code review, if there are no comments write: everything is fine, LGTM. If necessary, write a corrected version of the code.\n",
      "code lang: py\n",
      "code diff:\n",
      "[KEEP] \n",
      "[KEEP] from parlai.core.agents import Agent\n",
      "[KEEP] \n",
      "[DEL]from torch.autograd import Variable\n",
      "[KEEP] import torch.nn as nn\n",
      "[KEEP] import torch\n",
      "[KEEP] import os\n",
      "\n",
      "    ### Feedback and Suggestions (Response):\n",
      "\n",
      "\n",
      "Reference:\n",
      "everything is fine, LGTM.\n",
      "Prediction:\n",
      "Everything is fine, LGTM.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_response(prompt, model=\"gpt-3.5-turbo\", max_tokens=500, temperature=0.7, top_p=1.0):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    return response.choices[0].message['content'].strip()\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "# Collect qualitative results\n",
    "qualitative_results = []\n",
    "\n",
    "print(\"Generating qualitative summaries...\")\n",
    "for i in tqdm(range(len(test_dataset.select(range(2)))), desc=\"Generating Summaries\"):\n",
    "    sample = test_dataset[i]\n",
    "    formatted_prompt = format_example_for_task(sample)\n",
    "    prompt = formatted_prompt\n",
    "    reference = sample['responce']\n",
    "\n",
    "    # Generate the model's prediction\n",
    "    prediction = generate_response(prompt)\n",
    "\n",
    "    # Append results for qualitative analysis\n",
    "    qualitative_results.append({\n",
    "        \"Prompt\": prompt,\n",
    "        \"Reference\": reference,\n",
    "        \"Prediction\": prediction,\n",
    "    })\n",
    "\n",
    "# Display qualitative comparisons\n",
    "print(\"\\nQualitative Results:\")\n",
    "for idx, sample in enumerate(qualitative_results[:2], start=1):  # Show the first 2 samples\n",
    "    print(f\"Sample {idx}\")\n",
    "    print(f\"Prompt:\\n{sample['Prompt']}\")\n",
    "    print(f\"Reference:\\n{sample['Reference']}\")\n",
    "    print(f\"Prediction:\\n{sample['Prediction']}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Set: 100%|██████████| 10/10 [00:04<00:00,  2.48it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model...\")\n",
    "for i in tqdm(range(len(test_dataset.select(range(10)))), desc=\"Processing Test Set\"):\n",
    "    sample = test_dataset[i]\n",
    "    formatted_prompt = format_example_for_task(sample)\n",
    "    prompt = formatted_prompt\n",
    "    reference = sample['responce']\n",
    "\n",
    "    # Generate model prediction\n",
    "    prediction = generate_response(prompt)\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    scores = rouge.compute(predictions=[prediction], references=[reference])\n",
    "\n",
    "    # Append all results\n",
    "    results.append({\n",
    "        \"Prompt\": prompt,\n",
    "        \"Reference\": reference,\n",
    "        \"Prediction\": prediction,\n",
    "        \"ROUGE-1\": scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": scores[\"rougeL\"],\n",
    "        \"ROUGE-Lsum\": scores[\"rougeLsum\"]\n",
    "    })\n",
    "\n",
    "# Create a DataFrame for results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort results by ROUGE-Lsum\n",
    "results_sorted = results_df.sort_values(by=\"ROUGE-Lsum\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rouge 1 score:  0.41632478632478626\n",
      "Average rouge 2 score:  0.4005181347150259\n",
      "Average rouge L score:  0.4158119658119658\n",
      "Average rougeLSum score:  0.41529914529914524\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame for export\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Compute average ROUGE scores\n",
    "average_rouge1 = results_df[\"ROUGE-1\"].mean()\n",
    "average_rouge2 = results_df[\"ROUGE-2\"].mean()\n",
    "average_rougeL = results_df[\"ROUGE-L\"].mean()\n",
    "average_rougeLsum = results_df[\"ROUGE-Lsum\"].mean()\n",
    "print(\"Average rouge 1 score: \", average_rouge1)\n",
    "print(\"Average rouge 2 score: \", average_rouge2)\n",
    "print(\"Average rouge L score: \", average_rougeL)\n",
    "print(\"Average rougeLSum score: \", average_rougeLsum)# Create a DataFrame for export\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"gpt_patch_dataset_evaluation.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
